{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>div.item {\n",
       "    vertical-align: bottom;\n",
       "    text-align: center;\n",
       "}\n",
       "img {\n",
       "    background-color: white;\n",
       "}\n",
       ".caption {\n",
       "    display: block;\n",
       "}\n",
       "\n",
       "/* Three image containers (use 25% for four, and 50% for two, etc) */\n",
       ".column {\n",
       "  float: left;\n",
       "  width: 50%;\n",
       "  padding: 5px;\n",
       "}\n",
       "\n",
       "/* Clear floats after image containers */\n",
       ".row::after {\n",
       "  content: \"\";\n",
       "  clear: both;\n",
       "  display: table;\n",
       "}\n",
       "</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### import utils\n",
    "import warnings, utils\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "utils.set_css_style('style.css')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Logistic Regression\n",
    "\n",
    "Logistic regression is a classification algorithm used to assign observations to a discrete set of classes (a small number of discrete values). Logistic regression is a classification model that brings great explicability between different variables, just like linear regression. Some of the examples of classification problems are Email spam or not spam, Online transactions Fraud or not Fraud, Tumor Malignant or Benign. For now, we will focus on the binary classification problem in which y can take on only two values, 0 and 1. \n",
    "\n",
    "Most of the notions that will be presented will also generalize to the multiple-class case.\n",
    "\n",
    "For instance, if we are trying to build a spam classifier for email, then $x^{(i)}$ may be some feature of a piece of email, and y will be $1$ if the mail is spam, and 0 otherwise. Hence, $y \\in \\{0,1\\}$. $0$ is also called the negative class, and $1$ the positive class. They are sometimes also denoted by the symbols \"-\" and \"+\". Given $x^{(i)}$, the corresponding $y^{(i)}$ is also called the label for the training example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Hypothesis Representation\n",
    "\n",
    "We could approach the classification problem ignoring the fact that $y$ is discrete-valued, and use our old linear regression algorithm to try to predict $y$ given $x$. However, it is easy to construct examples where this method performs very poorly.\n",
    "\n",
    "<img src=\"figures/logistic_regression.png\" alt=\"logistic_regression\" style=\"width: 600px;\"/>\n",
    "\n",
    "Intuitively, it also doesn’t make sense for $h_{\\boldsymbol{\\theta}} (x)$ to take values larger than 1 or smaller than 0 when we know that $y \\in \\{0, 1\\}$. To fix this, let’s change the form for our hypotheses $h_{\\boldsymbol{\\theta}}(x)$ to satisfy $0 \\leq h_{\\boldsymbol{\\theta}} (x) \\leq 1$. In order to map predicted values to probabilities, we use the Sigmoid function (Logistic function). The function maps any real value into a value between 0 and 1. In machine learning, we use sigmoid to map predictions to probabilities.\n",
    "\n",
    "Our new form uses the \"Sigmoid Function,\" also called the \"Logistic Function\":\n",
    "\n",
    "\\begin{align*} & z = \\boldsymbol{\\theta}^\\top \\mathbf{x} \\newline & g(z) = \\dfrac{1}{1 + e^{-z}} \\newline & h_{\\boldsymbol{\\theta}} (\\mathbf{x}) = g ( \\boldsymbol{\\theta}^\\top \\mathbf{x} ) \\end{align*}\n",
    "\n",
    "Using one line to express our hypothesis:\n",
    "\n",
    "\\begin{equation}\n",
    "h_{\\boldsymbol{\\theta}} (\\mathbf{x}) = \\dfrac{1}{1 + e^{-\\boldsymbol{\\theta}^\\top \\mathbf{x}}} = \\dfrac{1}{1 + e^{-(\\theta_0 + \\theta_1 x_1 + \\theta_2 x_2 + ... + \\theta_n x_n)}}\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "Where $\\boldsymbol{\\theta} = \\begin{bmatrix}\\theta_0 \\newline \\theta_1 \\newline \\vdots \\newline \\theta_n\\end{bmatrix}$ and $\\mathbf{x} = \\begin{bmatrix}x_0 \\newline x_1 \\newline \\vdots \\newline x_n\\end{bmatrix}$\n",
    "\n",
    "Remark: we assume $x_{0}^{(i)} =1 \\text{ for } (i\\in { 1,\\dots, m } )$. This allows us to do matrix operations with $\\boldsymbol{\\theta}$ and $\\mathbf{x}$.\n",
    "\n",
    "The following image shows us what the sigmoid function $g(z) = \\dfrac{1}{1 + e^{-z}}$ looks like:\n",
    "\n",
    "<img src=\"figures/sigmoid.svg.png\" alt=\"sigmoid\" style=\"width: 400px;\"/>\n",
    "\n",
    "\n",
    "$h_{\\boldsymbol{\\theta}}(x)$ will give us the probability that our output is 1. For example, $h_{\\boldsymbol{\\theta}}(\\mathbf{x})=0.7$ gives us a probability of 70% that our output is 1. Our probability that our prediction is 0 is just the complement of our probability that it is 1 (e.g. if the probability that it is 1 is 70%, then the probability that it is 0 is 30%).\n",
    "\n",
    "\\begin{align*}& h_{\\boldsymbol{\\theta}}(\\mathbf{x}) = p(y=1 | \\mathbf{x} ; \\boldsymbol{\\theta}) = 1 - p(y=0 | \\mathbf{x} ; \\boldsymbol{\\theta}) \\newline& p(y = 0 | \\mathbf{x} ; \\boldsymbol{\\theta}) + p(y = 1 | \\mathbf{x} ; \\boldsymbol{\\theta}) = 1\\end{align*}\n",
    "\n",
    "### Logit Function\n",
    "\n",
    "Logistic regression can be expressed as:\n",
    "\n",
    "\\begin{equation}\n",
    "log(\\frac{p(y = 1 | \\mathbf{x} ; \\boldsymbol{\\theta})}{1- p(y = 1 | \\mathbf{x} ; \\boldsymbol{\\theta}) }) = \\boldsymbol{\\theta}^\\top \\mathbf{x}\n",
    "\\end{equation}\n",
    "\n",
    "Where the left-hand side is called the logit or log-odds function.\n",
    "The odds signifies the ratio of the probability of success to the probability of failure. Therefore, in Logistic Regression, a linear combination of inputs is mapped to the log(odds)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Decision Boundary\n",
    "\n",
    "When we pass the inputs through a hypothesis function, it returns a probability score between 0 and 1. We expect our classifier to give us a set of outputs or classes based on the probability.\n",
    "\n",
    "It's great that we can obtain a probability from our logistic regression model. However, at the end of the day sometimes users just want a simple decision to be made for them, for their real-world problems. Should the email be sent to the spam folder or not? Should the loan be approved or not? Which road should we route the user through? How can we use our probability estimate to help the tool using our model to make a decision? We choose a **threshold**.\n",
    "\n",
    "If we have 2 classes, we basically decide with a threshold value on the output probability above which we classify values into the positive class and if the value goes below the threshold then we classify it into the negative class.\n",
    "\n",
    "In order to get our discrete 0 or 1 classification, one way is to consider a threshold of 0.5, we may translate the output of the hypothesis function in this case as follows:\n",
    "\n",
    "\\begin{align*}& h_{\\boldsymbol{\\theta}}(\\mathbf{x}) \\geq 0.5 \\rightarrow y = 1 \\newline& h_{\\boldsymbol{\\theta}}(\\mathbf{x}) < 0.5 \\rightarrow y = 0 \\newline\\end{align*}\n",
    "\n",
    "<img src=\"figures/decision_boundary.png\" alt=\"decision_boundary\" style=\"width: 400px;\"/>\n",
    "\n",
    "The way our logistic function g behaves is that when its input is greater than or equal to zero, its output is greater than or equal to 0.5 (See sigmoid function above):\n",
    "\n",
    "\\begin{align*}& h_{\\boldsymbol{\\theta}}(\\mathbf{x}) = g(\\boldsymbol{\\theta}^\\top \\mathbf{x}) \\geq 0.5 \\newline& \\text{when} \\; \\boldsymbol{\\theta}^\\top \\mathbf{x} \\geq 0\\end{align*}\n",
    "\n",
    "From these statements we can now say:\n",
    "\n",
    "\\begin{align*}& \\boldsymbol{\\theta}^\\top \\mathbf{x} \\geq 0 \\Rightarrow y = 1 \\newline& \\boldsymbol{\\theta}^\\top \\mathbf{x} < 0 \\Rightarrow y = 0 \\newline\\end{align*}\n",
    "\n",
    "The **decision boundary** for this threshold is the line $\\boldsymbol{\\theta}^\\top \\mathbf{x} = 0$ that separates the area where y = 0 and where y = 1.\n",
    "\n",
    "So far, we have considered a simple threshold of a binary classification problem in which all probabilities less than or equal to 50 percent are classified into the positive class and all probabilities greater than 50 percent are classified into the negative class. However, for certain real-world problems, this threshold could be different. Depending on how we want the balance of our recall and precision, our false positives and false negatives... We can tune our choice of threshold to optimize the metric of our choice on a separate validation set. \n",
    "\n",
    "The Area-Under-Curve (AUC) of the ROC curve provides an aggregate measure of performance across all possible classification thresholds. AUC helps us choose between models when we don't know what decision threshold is going to be ultimately used. AUC is interpreted as follows: if we pick a random positive and a random negative what's the probability my model scores them in the correct relative order?\n",
    "\n",
    "(For more info, go back to evaluation metrics in Chapter 2)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Cost Function\n",
    "\n",
    "We learned about the cost function J(θ) in the Linear regression, the cost function represents optimization objective i.e. we create a cost function and minimize it so that we can develop an accurate model with minimum error.\n",
    "\n",
    "If we try to use the cost function of the linear regression in ‘Logistic Regression’ then it would be of no use as it would end up being a **non-convex** function with **many local minimums**, in which it would be very difficult to minimize the cost value and find the global minimum.\n",
    "\n",
    "Instead, our cost function for logistic regression looks like:\n",
    "\n",
    "\\begin{align*}\n",
    "J(\\boldsymbol{\\theta}) & = \\dfrac{1}{m} \\sum_{i=1}^m \\mathrm{Cost}(h_{\\boldsymbol{\\theta}}(\\mathbf{x}^{(i)}),y^{(i)}) \\newline\n",
    "& = \\dfrac{1}{m} \\sum_{i=1}^m \\left[-y^{(i)} \\log(h_{\\boldsymbol{\\theta}}(\\mathbf{x}^{(i)})) -(1-y^{(i)}) \\log(1-h_{\\boldsymbol{\\theta}}(\\mathbf{x}^{(i)}))\\right]\n",
    "\\end{align*}\n",
    "\n",
    "The cost can be written as:\n",
    "\n",
    "\\begin{align*}\n",
    "& \\mathrm{Cost}(h_{\\boldsymbol{\\theta}}(\\mathbf{x}),y) = -\\log(h_{\\boldsymbol{\\theta}}(\\mathbf{x})) \\; & \\text{if y = 1} \\newline \n",
    "& \\mathrm{Cost}(h_{\\boldsymbol{\\theta}}(\\mathbf{x}),y) = -\\log(1-h_{\\boldsymbol{\\theta}}(\\mathbf{x})) \\; & \\text{if y = 0}\n",
    "\\end{align*}\n",
    "\n",
    "If our correct answer 'y' is 0, then the cost function will be 0 if our hypothesis function also outputs 0. If our hypothesis approaches 1, then the cost function will approach infinity.\n",
    "\n",
    "If our correct answer 'y' is 1, then the cost function will be 0 if our hypothesis function outputs 1. If our hypothesis approaches 0, then the cost function will approach infinity.\n",
    "\n",
    "<div class=\"row\">\n",
    "  <div class=\"column\">\n",
    "    <img src=\"figures/cost_1.png\" alt=\"cost_1\" style=\"width: 400px;\"/>\n",
    "  </div>\n",
    "  <div class=\"column\">\n",
    "     <img src=\"figures/cost_0.png\" alt=\"cost_0\" style=\"width: 400px;\"/>\n",
    "  </div>\n",
    "</div>\n",
    "\n",
    "Unlike mean squared error, there is less emphasis on errors where the output is relatively close to the label where it's almost linear compared to quadratic. However, also unlike mean squared error, this loss function grows exponentially when the prediction is close to the opposite the label. In other words, there is a very high penalty when the model not only gets it wrong but does so with very high confidence. \n",
    "\n",
    "Note that writing the cost function in this way guarantees that $J(\\boldsymbol{\\theta})$ is convex for logistic regression.\n",
    "\n",
    "A vectorized implementation is:\n",
    "\n",
    "\\begin{align*} & h = g(X \\boldsymbol{\\theta})\\newline & J(\\boldsymbol{\\theta}) = \\frac{1}{m} \\cdot \\left(-y^{T}\\log(h)-(1-y)^{T}\\log(1-h)\\right) \\end{align*}\n",
    "\n",
    "The loss is called the **log-loss**. It is also known by the **log-likelihood**, or the **cross-entropy** loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Gradient Descent\n",
    "\n",
    "Now the question arises, how do we reduce the cost value. Well, this can be done by using Gradient Descent. The main goal of Gradient descent is to minimize the cost value. i.e. min $J(\\boldsymbol{\\theta})$.\n",
    "\n",
    "Remember that the general form of gradient descent is:\n",
    "\n",
    "$$\n",
    "\\begin{align*}& Repeat \\; \\lbrace \\newline & \\; \\theta_j := \\theta_j - \\alpha \\dfrac{\\partial}{\\partial \\theta_j}J(\\boldsymbol{\\theta}) \\newline & \\rbrace\\end{align*}\n",
    "$$\n",
    "\n",
    "We can work out the derivative part using calculus to get:\n",
    "\n",
    "\\begin{align*} & Repeat \\; \\lbrace \\newline & \\; \\theta_j := \\theta_j - \\frac{\\alpha}{m} \\sum_{i=1}^m (h_{\\boldsymbol{\\theta}}(\\mathbf{x}^{(i)}) - y^{(i)}) \\mathbf{x}_j^{(i)} \\newline & \\rbrace \\end{align*}\n",
    "\n",
    "\n",
    "\"Conjugate gradient\", \"BFGS\", and \"L-BFGS\" are more sophisticated, sometimes faster ways to optimize $\\boldsymbol{\\theta}$ that can be used instead of gradient descent. For more details [check here](https://stackoverflow.com/questions/38640109/logistic-regression-python-solvers-defintions)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Regularization** is **very important** in logistic regression because driving the **log-loss** to zero is difficult and dangerous. First, as gradient descent seeks to minimize the log-loss, it pushes output values closer to one for positive labels and closer to zero for negative labels. Due to the equation of the sigmoid, the function asymptotes to zero when the logit function is negative infinity, and to one when the logit is positive infinity. To get the logits to negative or positive infinity, the weights are highly increased leading to numerical stability problems, overflows, and underflows. This is dangerous and can ruin our training. Also, near the asymptotes, as you can see from the graph, the sigmoid function becomes flatter and flatter. This means that the derivative is getting closer and closer to zero. Since we use the derivatives/gradients to update the weights, it is important for the gradients not to become zero, or else, training will stop. This is called saturation, or a vanishing gradient problem and makes training difficult.\n",
    "\n",
    "It is also extremely important that our model generalizes so that we have the best predictions on new data which is the entire reason we create the model, to begin with. To help do this, it is important that we do not overfit our data. Therefore, adding in penalty terms to the objective function like with $L_1$ regularization for sparsity and $L_2$ regularization for keeping model weights small, or adding early stopping can help in this regard."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Multiclass Classification\n",
    "\n",
    "## 6.1. One-vs-all\n",
    "\n",
    "Now we will approach the classification of data when we have more than two categories. Instead of $y = \\{0,1\\}$ we will expand our definition so that $y = \\{1,2,...C\\}$.\n",
    "\n",
    "Since $y = \\{1,2,...C\\}$, we divide our problem into $C$ binary classification problems; in each one, we predict the probability that 'y' is a member of one of our classes.\n",
    "\n",
    "\\begin{align*}\n",
    "& y \\in \\lbrace1, 2 ... C\\rbrace \\newline\n",
    "& h_{\\boldsymbol{\\theta}}^{(1)}(\\mathbf{x}) = p(y = 1 | \\mathbf{x} ; \\boldsymbol{\\theta}^{(1)}) \\newline\n",
    "& h_{\\boldsymbol{\\theta}}^{(2)}(\\mathbf{x}) = p(y = 2 | \\mathbf{x} ; \\boldsymbol{\\theta}^{(2)}) \\newline\n",
    "& \\cdots \\newline& h_{\\boldsymbol{\\theta}}^{(C)}(\\mathbf{x}) = p(y =  C| \\mathbf{x} ; \\boldsymbol{\\theta}^{(C)}) \\newline\n",
    "& \\mathrm{prediction} = \\max_{c \\in \\{1,2,...C\\}}( h_{\\boldsymbol{\\theta}} ^{(c)}(\\mathbf{x}) )\\newline\n",
    "\\end{align*}\n",
    "\n",
    "We are basically choosing one class and then lumping all the others into a single second class. We do this repeatedly, applying binary logistic regression to each case, and then use the hypothesis that returned the highest value as our prediction.\n",
    "\n",
    "The following image shows how one could classify 3 classes:\n",
    "\n",
    "<img src=\"figures/one-vs-all.png\" alt=\"one-vs-all\" style=\"width: 600px;\"/>\n",
    "\n",
    "**To summarize**: Train a logistic regression classifier $h_{\\boldsymbol{\\theta}}(\\mathbf{x})$ for each class $c$ to predict the probability that $p(y = c)$. To make a prediction on a new $\\mathbf{x}$, pick the class that maximizes $h_{\\boldsymbol{\\theta}}(\\mathbf{x})$\n",
    "\n",
    "Although this seems like a great solution, it comes with several problems. First, the scale of the confidence score might be different for each of the binary classification models, which biases our overall prediction. However, even if that isn't the case, each of the binary classification models, see very unbalanced data distributions since, for each one, the negative class is the sum of all the other classes, besides the one that is currently marked for the positive class. \n",
    "\n",
    "A possible fix for this imbalance problem is the one-verse-one method. Here, instead of having a model for each class, there is a model for each binary combination of the classes. If there are $n$ classes, this means that there would be $n(n-1)/2$ models, so in the order of $n^2$. If we have four classes, that would be six models, but if we have a thousand classes, like in many image classification use cases, there would be 499,500 models. Each model essentially outputs a vote for its predicted label. Then all the votes are accumulated and the class that has the most wins. However, this doesn't fix the ambiguity problem for the possibility of having the same number of votes for different classes.\n",
    "\n",
    "## 6.2. Multinomial Logestic Regression (Softmax Regression)\n",
    "\n",
    "### Hypothesis Representation\n",
    "\n",
    "Softmax regression (or multinomial logistic regression) is a generalization of logistic regression to the case where we want to handle multiple classes. In logistic regression we assumed that the labels were binary: $y^{(i)}\\in\\{0,1\\}$. Softmax regression allows us to handle $y^{(i)}\\in\\{1,…,C\\}$ where C is the number of classes. Note that our convention will be to index the classes starting from 1, rather than from 0.\n",
    "\n",
    "Given an input $\\mathbf{x}$, we want our hypothesis to estimate the probability that $p(y=c|\\mathbf{x})$ for each value of $c=\\{1,…,C\\}$. I.e., we want to estimate the probability of the class label taking on each of the $C$ different possible values. Thus, our hypothesis will output a C-dimensional vector (whose elements sum to 1) giving us our $C$ estimated probabilities. Concretely, our hypothesis $\\mathbf{h}_{\\Theta}(x)$ takes the form (Notice that our hypothesis function is now written in bold to demonstrate it's a vector $\\mathbf{h}$):\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{h}_{\\Theta}(\\mathbf{x}) =\n",
    "\\begin{bmatrix}\n",
    "h_{{\\Theta}}^{(1)}(\\mathbf{x}) \\\\\n",
    "h_{{\\Theta}}^{(2)}(\\mathbf{x}) \\\\\n",
    "\\vdots \\\\\n",
    "h_{{\\Theta}}^{(C)}(\\mathbf{x}) \n",
    "\\end{bmatrix} =\n",
    "\\begin{bmatrix}\n",
    "p(y = 1 | \\mathbf{x}; {\\Theta}) \\\\\n",
    "p(y = 2 | \\mathbf{x}; {\\Theta}) \\\\\n",
    "\\vdots \\\\\n",
    "p(y = C | \\mathbf{x}; {\\Theta}) \n",
    "\\end{bmatrix} =\n",
    "\\frac{1}{ \\sum_{c=1}^{C}{e^{\\boldsymbol{\\theta}^{(c)\\top} \\mathbf{x}} }}\n",
    "\\begin{bmatrix}\n",
    "e^{\\boldsymbol{\\theta}^{(1)\\top} \\mathbf{x} } \\\\\n",
    "e^{\\boldsymbol{\\theta}^{(2)\\top} \\mathbf{x} } \\\\\n",
    "\\vdots \\\\\n",
    "e^{\\boldsymbol{\\theta}^{(C)\\top} \\mathbf{x} } \\\\\n",
    "\\end{bmatrix}\n",
    "\\end{equation}\n",
    "\n",
    "Here $\\boldsymbol{\\theta}^{(1)} , \\boldsymbol{\\theta}^{(2)}, ..., \\boldsymbol{\\theta}^{(C)} \\in \\mathbb{R}^n$ are the parameters of our model. Notice that the term $\\sum_{c=1}^{C} {e^{\\boldsymbol{\\theta}^{(c)\\top} \\mathbf{x}}}$ normalizes the distribution, so that it sums to one.\n",
    "\n",
    "For convenience, we will also write ${\\Theta}$ to denote all the parameters of our model. When you implement softmax regression, it is usually convenient to represent ${\\Theta}$ as a (n+1)-by-C matrix obtained by concatenating $\\boldsymbol{\\theta}^{(1)} , \\boldsymbol{\\theta}^{(2)}, ..., \\boldsymbol{\\theta}^{(C)} $ into columns, so that\n",
    "\n",
    "\\begin{equation}\n",
    "\\Theta = \\left[\\begin{array}{cccc}| & | & | & | \\\\\n",
    "\\boldsymbol{\\theta}^{(1)} & \\boldsymbol{\\theta}^{(2)} & \\cdots & \\boldsymbol{\\theta}^{(C)} \\\\\n",
    "| & | & | & |\n",
    "\\end{array}\\right].\n",
    "\\end{equation}\n",
    "\n",
    "### Cost Function\n",
    "We now describe the cost function that we’ll use for softmax regression. In the equation below, $\\mathbb{1}_{\\left\\{.\\right\\}}$ is the \"indicator function\", so that $\\mathbb{1}_{\\left\\{\\text{a true statement}\\right\\}}=1$, and $\\mathbb{1}_{\\left\\{\\text{a false statement}\\right\\}}$. For example, $\\mathbb{1}_{\\left\\{\\text{2+2=4}\\right\\}}$ evaluates to 1; whereas $\\mathbb{1}_{\\left\\{\\text{1+1=5}\\right\\}}$ evaluates to 0. Our cost function will be:\n",
    "\n",
    "\\begin{align}\n",
    "J(\\Theta) = - \\left[ \\sum_{i=1}^{m} \\sum_{c=1}^{C}  \\mathbb{1}_{\\left\\{y^{(i)} = c\\right\\}} \\log p(y^{(i)} = c | \\mathbf{x}^{(i)} ; \\Theta)\\right]\n",
    "\\end{align}\n",
    "\n",
    "Notice that this generalizes the logistic regression cost function, which could also have been written:\n",
    "\n",
    "\\begin{align}\n",
    "J(\\boldsymbol{\\theta}) &= - \\left[ \\sum_{i=1}^m   (1-y^{(i)}) \\log (1-h_\\boldsymbol{\\theta}(\\mathbf{x}^{(i)})) + y^{(i)} \\log h_\\boldsymbol{\\theta}(\\mathbf{x}^{(i)}) \\right] \\\\\n",
    "&= - \\left[ \\sum_{i=1}^{m} \\sum_{c=0}^{1} \\mathbb{1}_{\\left\\{y^{(i)} = c\\right\\}} \\log p(y^{(i)} = c | \\mathbf{x}^{(i)} ; \\boldsymbol{\\theta}) \\right]\n",
    "\\end{align}\n",
    "\n",
    "The softmax cost function is similar, except that we now sum over the C different possible values of the class label. Note also that in softmax regression, we have that\n",
    "\n",
    "$$\n",
    "p(y^{(i)} = c | \\mathbf{x}^{(i)} ; \\Theta) = \\frac{e^{\\boldsymbol{\\theta}^{(c)\\top} x^{(i)}}}{\\sum_{c^\\prime=1}^C e^{\\boldsymbol{\\theta}^{(c^\\prime)\\top} \\mathbf{x}^{(i)}} }\n",
    "$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Logistic Regression with Scikit-Learn\n",
    "\n",
    "## The dataset\n",
    "\n",
    "The dataset we'll be using contain the results of a virtual driving simulation of vehicles with an AD system. Based on the results of these simulations, we would like to predict for a new vehicle, whether the distance to the vehicle in front is respected and whether the safety distance is kept at all times, namely 2 seconds.\n",
    "\n",
    "This variable is therefore binary (Did the vehicle keep the safety distance? Yes or no). We call it a warning.\n",
    "\n",
    "The explanatory variables are called metadata. These are variables entered in the simulation scenario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Package\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "from sklearn import metrics\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading the data in csv using `pandas`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metadata dataset size (19992, 13)\n",
      "warnings dataset size (19990, 4)\n"
     ]
    }
   ],
   "source": [
    "metadata = pd.read_csv('Data/metadata.csv')\n",
    "warnings = pd.read_csv('Data/warnings.csv')\n",
    "print('metadata dataset size', str(metadata.shape))\n",
    "print('warnings dataset size', str(warnings.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>scenario_id</th>\n",
       "      <th>AccelDur_s</th>\n",
       "      <th>LaneWidth_m</th>\n",
       "      <th>PVSpeed_kmh</th>\n",
       "      <th>PVPosit_m</th>\n",
       "      <th>PVAccel_ms2</th>\n",
       "      <th>PVDecel_ms2</th>\n",
       "      <th>PVCar</th>\n",
       "      <th>PVBus</th>\n",
       "      <th>PVMoto</th>\n",
       "      <th>Slope-6</th>\n",
       "      <th>Slope0</th>\n",
       "      <th>Slop6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5bc4920704c87d00012789f3</td>\n",
       "      <td>1.4</td>\n",
       "      <td>3.5</td>\n",
       "      <td>82.3</td>\n",
       "      <td>584.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5bc4920704c87d00012789f4</td>\n",
       "      <td>1.9</td>\n",
       "      <td>3.5</td>\n",
       "      <td>97.3</td>\n",
       "      <td>563.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>2.8</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5bc4920704c87d00012789f5</td>\n",
       "      <td>2.6</td>\n",
       "      <td>3.5</td>\n",
       "      <td>82.0</td>\n",
       "      <td>565.0</td>\n",
       "      <td>2.2</td>\n",
       "      <td>2.1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5bc4920704c87d00012789f6</td>\n",
       "      <td>1.8</td>\n",
       "      <td>3.5</td>\n",
       "      <td>83.3</td>\n",
       "      <td>687.0</td>\n",
       "      <td>2.2</td>\n",
       "      <td>1.2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5bc4920704c87d00012789f7</td>\n",
       "      <td>2.6</td>\n",
       "      <td>3.5</td>\n",
       "      <td>102.7</td>\n",
       "      <td>562.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>2.5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                scenario_id  AccelDur_s  LaneWidth_m  PVSpeed_kmh  PVPosit_m  \\\n",
       "0  5bc4920704c87d00012789f3         1.4          3.5         82.3      584.0   \n",
       "1  5bc4920704c87d00012789f4         1.9          3.5         97.3      563.0   \n",
       "2  5bc4920704c87d00012789f5         2.6          3.5         82.0      565.0   \n",
       "3  5bc4920704c87d00012789f6         1.8          3.5         83.3      687.0   \n",
       "4  5bc4920704c87d00012789f7         2.6          3.5        102.7      562.0   \n",
       "\n",
       "   PVAccel_ms2  PVDecel_ms2  PVCar  PVBus  PVMoto  Slope-6  Slope0  Slop6  \n",
       "0          2.0          2.2      1      0       0        0       1      0  \n",
       "1          1.5          2.8      1      0       0        0       1      0  \n",
       "2          2.2          2.1      1      0       0        0       1      0  \n",
       "3          2.2          1.2      1      0       0        0       1      0  \n",
       "4          1.4          2.5      1      0       0        0       1      0  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>scenario_id</th>\n",
       "      <th>longitudinal_deceleration</th>\n",
       "      <th>safety_distance_m</th>\n",
       "      <th>safety_distance_s</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5bc4920704c87d00012789f3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5bc4920704c87d00012789f4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5bc4920704c87d00012789f5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5bc4920704c87d00012789f6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5bc4920704c87d00012789f7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                scenario_id  longitudinal_deceleration  safety_distance_m  \\\n",
       "0  5bc4920704c87d00012789f3                        1.0                0.0   \n",
       "1  5bc4920704c87d00012789f4                        0.0                1.0   \n",
       "2  5bc4920704c87d00012789f5                        1.0                0.0   \n",
       "3  5bc4920704c87d00012789f6                        1.0                0.0   \n",
       "4  5bc4920704c87d00012789f7                        0.0                1.0   \n",
       "\n",
       "   safety_distance_s  \n",
       "0                0.0  \n",
       "1                1.0  \n",
       "2                0.0  \n",
       "3                0.0  \n",
       "4                1.0  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "warnings.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From `warnings` dataframe, we will only keep \"safety_distance_s\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings = warnings[[\"scenario_id\",\"safety_distance_s\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's join the dataframes together using the common key `scenario_id`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = metadata.merge(warnings, left_on='scenario_id', right_on='scenario_id')\n",
    "df = df.set_index('scenario_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AccelDur_s</th>\n",
       "      <th>LaneWidth_m</th>\n",
       "      <th>PVSpeed_kmh</th>\n",
       "      <th>PVPosit_m</th>\n",
       "      <th>PVAccel_ms2</th>\n",
       "      <th>PVDecel_ms2</th>\n",
       "      <th>PVCar</th>\n",
       "      <th>PVBus</th>\n",
       "      <th>PVMoto</th>\n",
       "      <th>Slope-6</th>\n",
       "      <th>Slope0</th>\n",
       "      <th>Slop6</th>\n",
       "      <th>safety_distance_s</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>scenario_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5bc4920704c87d00012789f3</th>\n",
       "      <td>1.4</td>\n",
       "      <td>3.5</td>\n",
       "      <td>82.3</td>\n",
       "      <td>584.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5bc4920704c87d00012789f4</th>\n",
       "      <td>1.9</td>\n",
       "      <td>3.5</td>\n",
       "      <td>97.3</td>\n",
       "      <td>563.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>2.8</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5bc4920704c87d00012789f5</th>\n",
       "      <td>2.6</td>\n",
       "      <td>3.5</td>\n",
       "      <td>82.0</td>\n",
       "      <td>565.0</td>\n",
       "      <td>2.2</td>\n",
       "      <td>2.1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5bc4920704c87d00012789f6</th>\n",
       "      <td>1.8</td>\n",
       "      <td>3.5</td>\n",
       "      <td>83.3</td>\n",
       "      <td>687.0</td>\n",
       "      <td>2.2</td>\n",
       "      <td>1.2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5bc4920704c87d00012789f7</th>\n",
       "      <td>2.6</td>\n",
       "      <td>3.5</td>\n",
       "      <td>102.7</td>\n",
       "      <td>562.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>2.5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          AccelDur_s  LaneWidth_m  PVSpeed_kmh  PVPosit_m  \\\n",
       "scenario_id                                                                 \n",
       "5bc4920704c87d00012789f3         1.4          3.5         82.3      584.0   \n",
       "5bc4920704c87d00012789f4         1.9          3.5         97.3      563.0   \n",
       "5bc4920704c87d00012789f5         2.6          3.5         82.0      565.0   \n",
       "5bc4920704c87d00012789f6         1.8          3.5         83.3      687.0   \n",
       "5bc4920704c87d00012789f7         2.6          3.5        102.7      562.0   \n",
       "\n",
       "                          PVAccel_ms2  PVDecel_ms2  PVCar  PVBus  PVMoto  \\\n",
       "scenario_id                                                                \n",
       "5bc4920704c87d00012789f3          2.0          2.2      1      0       0   \n",
       "5bc4920704c87d00012789f4          1.5          2.8      1      0       0   \n",
       "5bc4920704c87d00012789f5          2.2          2.1      1      0       0   \n",
       "5bc4920704c87d00012789f6          2.2          1.2      1      0       0   \n",
       "5bc4920704c87d00012789f7          1.4          2.5      1      0       0   \n",
       "\n",
       "                          Slope-6  Slope0  Slop6  safety_distance_s  \n",
       "scenario_id                                                          \n",
       "5bc4920704c87d00012789f3        0       1      0                0.0  \n",
       "5bc4920704c87d00012789f4        0       1      0                1.0  \n",
       "5bc4920704c87d00012789f5        0       1      0                0.0  \n",
       "5bc4920704c87d00012789f6        0       1      0                0.0  \n",
       "5bc4920704c87d00012789f7        0       1      0                1.0  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#verification merge\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19990, 13)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **describe()** method is used for calculating some statistical data like percentile, mean and std of the numerical values of the Series or DataFrame. It analyzes both numeric and object series and also the DataFrame column sets of mixed data types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AccelDur_s</th>\n",
       "      <th>LaneWidth_m</th>\n",
       "      <th>PVSpeed_kmh</th>\n",
       "      <th>PVPosit_m</th>\n",
       "      <th>PVAccel_ms2</th>\n",
       "      <th>PVDecel_ms2</th>\n",
       "      <th>PVCar</th>\n",
       "      <th>PVBus</th>\n",
       "      <th>PVMoto</th>\n",
       "      <th>Slope-6</th>\n",
       "      <th>Slope0</th>\n",
       "      <th>Slop6</th>\n",
       "      <th>safety_distance_s</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>19990.000000</td>\n",
       "      <td>19990.000000</td>\n",
       "      <td>19990.000000</td>\n",
       "      <td>19990.000000</td>\n",
       "      <td>19990.000000</td>\n",
       "      <td>19990.000000</td>\n",
       "      <td>19990.000000</td>\n",
       "      <td>19990.000000</td>\n",
       "      <td>19990.000000</td>\n",
       "      <td>19990.000000</td>\n",
       "      <td>19990.000000</td>\n",
       "      <td>19990.000000</td>\n",
       "      <td>19990.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2.523307</td>\n",
       "      <td>3.199970</td>\n",
       "      <td>85.232131</td>\n",
       "      <td>610.019210</td>\n",
       "      <td>1.995908</td>\n",
       "      <td>1.498624</td>\n",
       "      <td>0.333317</td>\n",
       "      <td>0.333317</td>\n",
       "      <td>0.333367</td>\n",
       "      <td>0.250025</td>\n",
       "      <td>0.249975</td>\n",
       "      <td>0.249975</td>\n",
       "      <td>0.469485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.448339</td>\n",
       "      <td>0.300008</td>\n",
       "      <td>14.469862</td>\n",
       "      <td>56.911916</td>\n",
       "      <td>0.578122</td>\n",
       "      <td>0.868606</td>\n",
       "      <td>0.471410</td>\n",
       "      <td>0.471410</td>\n",
       "      <td>0.471428</td>\n",
       "      <td>0.433038</td>\n",
       "      <td>0.433009</td>\n",
       "      <td>0.433009</td>\n",
       "      <td>0.499080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.900000</td>\n",
       "      <td>60.000000</td>\n",
       "      <td>508.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.300000</td>\n",
       "      <td>2.900000</td>\n",
       "      <td>72.700000</td>\n",
       "      <td>561.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2.500000</td>\n",
       "      <td>2.900000</td>\n",
       "      <td>85.400000</td>\n",
       "      <td>610.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>3.800000</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>97.700000</td>\n",
       "      <td>659.000000</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>2.300000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>5.000000</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>110.000000</td>\n",
       "      <td>712.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         AccelDur_s   LaneWidth_m   PVSpeed_kmh     PVPosit_m   PVAccel_ms2  \\\n",
       "count  19990.000000  19990.000000  19990.000000  19990.000000  19990.000000   \n",
       "mean       2.523307      3.199970     85.232131    610.019210      1.995908   \n",
       "std        1.448339      0.300008     14.469862     56.911916      0.578122   \n",
       "min        0.000000      2.900000     60.000000    508.000000      1.000000   \n",
       "25%        1.300000      2.900000     72.700000    561.000000      1.500000   \n",
       "50%        2.500000      2.900000     85.400000    610.000000      2.000000   \n",
       "75%        3.800000      3.500000     97.700000    659.000000      2.500000   \n",
       "max        5.000000      3.500000    110.000000    712.000000      3.000000   \n",
       "\n",
       "        PVDecel_ms2         PVCar         PVBus        PVMoto       Slope-6  \\\n",
       "count  19990.000000  19990.000000  19990.000000  19990.000000  19990.000000   \n",
       "mean       1.498624      0.333317      0.333317      0.333367      0.250025   \n",
       "std        0.868606      0.471410      0.471410      0.471428      0.433038   \n",
       "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "25%        0.700000      0.000000      0.000000      0.000000      0.000000   \n",
       "50%        1.500000      0.000000      0.000000      0.000000      0.000000   \n",
       "75%        2.300000      1.000000      1.000000      1.000000      0.750000   \n",
       "max        3.000000      1.000000      1.000000      1.000000      1.000000   \n",
       "\n",
       "             Slope0         Slop6  safety_distance_s  \n",
       "count  19990.000000  19990.000000       19990.000000  \n",
       "mean       0.249975      0.249975           0.469485  \n",
       "std        0.433009      0.433009           0.499080  \n",
       "min        0.000000      0.000000           0.000000  \n",
       "25%        0.000000      0.000000           0.000000  \n",
       "50%        0.000000      0.000000           0.000000  \n",
       "75%        0.000000      0.000000           1.000000  \n",
       "max        1.000000      1.000000           1.000000  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation: splitting data into training and test sets\n",
    "\n",
    "We will start by preparing the data:\n",
    "    1. Creation of X and y\n",
    "    2. Splitting the data into training and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X: explanatory variables / y: variable to predict\n",
    "X = df.drop('safety_distance_s', axis=1)\n",
    "y = df[[\"safety_distance_s\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Création des sets d'apprentissage et de test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define our LogesticRegression, and fit it using the training set.\n",
    "\n",
    "The Logestic Regression model on `scikit-learn` has a number of arguments, let's list some of them:\n",
    "\n",
    "* **penalty**: {‘l1’, ‘l2’, ‘elasticnet’, ‘none’}, default=’l2’: Used to specify the norm used in the regularization penalization.\n",
    "* **C**: float, default=1.0: Inverse of regularization strength; must be a positive float. Smaller values specify stronger regularization.\n",
    "* **l1_ratio**: float, default=None:  The Elastic-Net mixing parameter, with 0 <= l1_ratio <= 1. Only used if penalty='elasticnet'.\n",
    "* **fit_intercept**: bool, default=True: Specifies if a constant (a.k.a. bias or intercept $\\theta_0$) should be added to the decision function.\n",
    "* **solver**: {‘newton-cg’, ‘lbfgs’, ‘liblinear’, ‘sag’, ‘saga’}, default=’lbfgs’: Optimization algorithm to use in the optimization problem.\n",
    "* **random_state**: int, RandomState instance, default=None: The seed of the pseudo random number generator to use when shuffling the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score=nan,\n",
       "             estimator=LogisticRegression(C=1.0, class_weight=None, dual=False,\n",
       "                                          fit_intercept=True,\n",
       "                                          intercept_scaling=1, l1_ratio=None,\n",
       "                                          max_iter=100, multi_class='auto',\n",
       "                                          n_jobs=None, penalty='l2',\n",
       "                                          random_state=42, solver='lbfgs',\n",
       "                                          tol=0.0001, verbose=0,\n",
       "                                          warm_start=False),\n",
       "             iid='deprecated', n_jobs=None,\n",
       "             param_grid={'C': [0.001, 0.01, 0.02, 0.03, 0.05, 0.08, 0.1, 0.2,\n",
       "                               0.3, 0.5, 1, 10],\n",
       "                         'penalty': ['l1', 'l2', 'none']},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring='accuracy', verbose=0)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## training the model\n",
    "## Let tune the 'penalty' and 'C' hyperparameters\n",
    "param_grid = {'penalty': ['l1', 'l2', 'none'],\n",
    "              'C': [0.001, 0.01, 0.02, 0.03, 0.05, 0.08, 0.1, 0.2, 0.3, 0.5, 1, 10]}\n",
    "\n",
    "logR = LogisticRegression(random_state=42, fit_intercept=True)\n",
    "search = GridSearchCV(logR, param_grid, scoring='accuracy', cv=5)\n",
    "search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The attribute \"coef_\" allows us to list the coefficients $\\theta_1$, $\\theta_2$, ... $\\theta_{12}$ of our 12 features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression(C=0.03, class_weight=None, dual=False, fit_intercept=True,\n",
      "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
      "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
      "                   random_state=42, solver='lbfgs', tol=0.0001, verbose=0,\n",
      "                   warm_start=False)\n"
     ]
    }
   ],
   "source": [
    "logR = search.best_estimator_\n",
    "print(logR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.07580927,  1.41879654,  0.03301151, -0.01501184, -1.17196083,\n",
       "         1.20249636,  0.05403447,  1.35385761,  0.01417974, -0.43617967,\n",
       "        -0.45736431,  0.20509376]])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logR.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also get our intercept $\\theta_0$ with the attribute \"intercept_\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.63172657])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logR.intercept_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predicting over training & testing datasets\n",
    "y_train_pred = logR.predict(X_train)\n",
    "y_test_pred = logR.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Printing the first 10 predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 1., 0., 1., 1., 0., 1., 0., 0., 1.])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_pred[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Model evaluation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are other ways of measuring model performance (precision, recall, F1 Score, ROC Curve, etc). The accuracy is very simple to obtain. To get the mean accuracy on the given training dataset, we can use the method `score`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7695276209533338"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logR.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the mean accuracy on the given test dataset, we can also use the method `score`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7907286976821745"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logR.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2624,  586],\n",
       "       [ 669, 2118]])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.confusion_matrix(y_test, y_test_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`scikit-learn` also offers a nice function to plot the confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x12b343790>"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "metrics.plot_confusion_matrix(logR, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recall\n",
    "\n",
    "The recall of each class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.81744548, 0.75995694])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.recall_score(y_test, y_test_pred, average=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision\n",
    "\n",
    "The precision of each class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.79684179, 0.78328402])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.precision_score(y_test, y_test_pred, average=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### F1score\n",
    "\n",
    "The f1score of each class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.80701215, 0.77144418])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.f1_score(y_test, y_test_pred, average=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Receiver Operating Characteristic - AUC\n",
    "\n",
    "In able to plot ROC or evaluate AUC, we need the probability outputs of our model instead of the final prediction.\n",
    "\n",
    "We call the `predict_proba` method rather than `predict` in order to obtain a list of probabilities which represent the likelihood that a sample falls under a given category. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = logR.predict_proba(X_test)\n",
    "malignant_probs = probs[:,1]\n",
    "\n",
    "# calculating roc arguments\n",
    "fpr, tpr, thresholds = metrics.roc_curve(y_test, malignant_probs)\n",
    "# calculating auc\n",
    "roc_auc = metrics.auc(fpr, tpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=[8,6])\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.plot(fpr, tpr, 'y', label = 'AUC = %0.2f' % roc_auc)\n",
    "plt.legend(loc = 'lower right')\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "Stanford, UFLDL Tutorial. (2013, 26 Sep). Deep Learning Tutorial: Softmax Regression. Retrieved from http://deeplearning.stanford.edu/tutorial/supervised/SoftmaxRegression/.\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
