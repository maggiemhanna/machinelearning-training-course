{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>div.item {\n",
       "    vertical-align: bottom;\n",
       "    text-align: center;\n",
       "}\n",
       "img {\n",
       "    background-color: white;\n",
       "}\n",
       ".caption {\n",
       "    display: block;\n",
       "}\n",
       "\n",
       "/* Three image containers (use 25% for four, and 50% for two, etc) */\n",
       ".column {\n",
       "  float: left;\n",
       "  width: 50%;\n",
       "  padding: 5px;\n",
       "}\n",
       "\n",
       "/* Clear floats after image containers */\n",
       ".row::after {\n",
       "  content: \"\";\n",
       "  clear: both;\n",
       "  display: table;\n",
       "}\n",
       "</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import utils\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "utils.set_css_style('style.css')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Hyperparameter Tuning\n",
    "\n",
    "Choosing the right hyperparameters for your machine learning algorithm is a crucial task, since it can make a big difference on the performance of a model. \n",
    "\n",
    "Machine Learning models are composed of two different types of parameters:\n",
    "\n",
    "* **Hyperparameters** = are all the parameters which can be arbitrarily set by the user before starting training (eg. Learning rate, regularization parameter, batch size in the mini-batch gradient descent...).\n",
    "\n",
    "* Model **parameters** = are instead learned during the model training (eg. weights in Linear Regression, Neural Networks...).\n",
    "\n",
    "The model parameters define how to use input data to get the desired output and are learned at training time. Instead, Hyperparameters determine how our model is structured in the first place.\n",
    "Hyperparameter tuning is a type of optimization problem. We have a set of hyperparameters and we aim to find the right combination of their values which can help us to find either the minimum (eg. loss) or the maximum (eg. accuracy) of a function.\n",
    "\n",
    "<img src=\"figures/hyperparameter-tuning.png\" alt=\"hyperparameter-tuning\" style=\"width: 500px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1. Grid search\n",
    "\n",
    "Grid search is a method by which we create sets of possible hyper-parameters values for each hyper-parameter, then test them against each other in a “grid.” \n",
    "\n",
    "The recipe below evaluates different $\\lambda$ values for the regularized linear regression algorithm we have seen above (a Linear Regression with an $L_2$ regularization is called Ridge Regression) on the standard diabetes dataset. This is a one-dimensional grid search.\n",
    "\n",
    "Grid Search can be implemented in Python using `scikit-learn` `GridSearchCV()` function. The `verbose` parameter dictates whether the function will print information as it runs, and the `cv` parameter refers to cross validation folds. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the diabetes datasets\n",
    "dataset = datasets.load_diabetes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset Description:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. _diabetes_dataset:\n",
      "\n",
      "Diabetes dataset\n",
      "----------------\n",
      "\n",
      "Ten baseline variables, age, sex, body mass index, average blood\n",
      "pressure, and six blood serum measurements were obtained for each of n =\n",
      "442 diabetes patients, as well as the response of interest, a\n",
      "quantitative measure of disease progression one year after baseline.\n",
      "\n",
      "**Data Set Characteristics:**\n",
      "\n",
      "  :Number of Instances: 442\n",
      "\n",
      "  :Number of Attributes: First 10 columns are numeric predictive values\n",
      "\n",
      "  :Target: Column 11 is a quantitative measure of disease progression one year after baseline\n",
      "\n",
      "  :Attribute Information:\n",
      "      - Age\n",
      "      - Sex\n",
      "      - Body mass index\n",
      "      - Average blood pressure\n",
      "      - S1\n",
      "      - S2\n",
      "      - S3\n",
      "      - S4\n",
      "      - S5\n",
      "      - S6\n",
      "\n",
      "Note: Each of these 10 feature variables have been mean centered and scaled by the standard deviation times `n_samples` (i.e. the sum of squares of each column totals 1).\n",
      "\n",
      "Source URL:\n",
      "https://www4.stat.ncsu.edu/~boos/var.select/diabetes.html\n",
      "\n",
      "For more information see:\n",
      "Bradley Efron, Trevor Hastie, Iain Johnstone and Robert Tibshirani (2004) \"Least Angle Regression,\" Annals of Statistics (with discussion), 407-499.\n",
      "(https://web.stanford.edu/~hastie/Papers/LARS/LeastAngle_2002.pdf)\n"
     ]
    }
   ],
   "source": [
    "print(dataset.DESCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set:  (353, 10) (353,)\n",
      "Testing set:  (89, 10) (89,)\n"
     ]
    }
   ],
   "source": [
    "# Split the data into 2 sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(dataset.data, dataset.target, test_size=0.2)\n",
    "print(\"Training set: \", X_train.shape, y_train.shape)\n",
    "print(\"Testing set: \", X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n",
      "RMSE:  55.08\n",
      "Best regularization parameter:  0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  30 out of  30 | elapsed:    0.0s finished\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# load the diabetes datasets\n",
    "dataset = datasets.load_diabetes()\n",
    "\n",
    "# prepare a range of alpha values to test\n",
    "grid = {'alpha': [1,0.1,0.01,0.001,0.0001,0]}\n",
    "\n",
    "# create and fit a ridge regression model, testing each alpha\n",
    "model = Ridge()\n",
    "gsearch = GridSearchCV(estimator=model, param_grid=grid, scoring=\"neg_mean_squared_error\", verbose=1)\n",
    "gsearch.fit(X_train, y_train)\n",
    "\n",
    "# summarize the results of the grid search\n",
    "print('RMSE: ', np.round(np.sqrt(-1*gsearch.best_score_), 2))\n",
    "print('Best regularization parameter: ', gsearch.best_estimator_.alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluating RMSE performance on the testing set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE:  54.37\n"
     ]
    }
   ],
   "source": [
    "rmse_test = gsearch.score(X_test, y_test)\n",
    "print('RMSE: ', np.round(np.sqrt(-1*rmse_test), 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2. Random search\n",
    "\n",
    "As its name suggests, Random Search uses random combinations of hyperparameters. This means that not all of the parameter values are tried, and instead, parameters will be sampled with fixed numbers of iterations given by `n_iter` in the `RandomizedSearchCV()` function.\n",
    "\n",
    "Random Search would be advised to use over Grid Search when the searching space is high meaning that there are more than 3 dimensions as Random Search is able to explore a wider hyperparameter space. In the below example, grid search only tested three unique values for each hyperperameter, whereas the random search tested 9 unique values for each. That means if one hyperparameter is more important than the others, random search will be better. Think of it this way: if hyperparameter 2 doesn’t really matter, then we would want 9 different hyperparameter 1 values to test instead of 3. The same holds true for higher dimensions (more hyperparameters).\n",
    "\n",
    "<div class=\"item\">\n",
    "    <img src=\"figures/gs-vs-rs.png\" alt=\"/gs-vs-rs\" width=\"600px\"/>\n",
    "    <span class=\"caption\">With grid search, nine trials only test three distinct places. With random search, all nine trails explore distinct values.</span>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this we will use a logistic regression (a linear algorithm used for classification) which has many different hyperparameters. For this example we will only consider these hyperparameters:\n",
    "\n",
    "* The penalty (The regularization type L1 or L2)\n",
    "* The C value (The regularization parameter)\n",
    "\n",
    "The data set we will be using is the classic and simple iris data set. First we need to import the things we need, as well as separate the target variable from the independent variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "iris = datasets.load_iris()\n",
    "features = iris.data\n",
    "target = iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set:  (120, 4) (120,)\n",
      "Testing set:  (30, 4) (30,)\n"
     ]
    }
   ],
   "source": [
    "# Split the 2 data into 2 sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state=1)\n",
    "print(\"Training set: \", X_train.shape, y_train.shape)\n",
    "print(\"Testing set: \", X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.98\n",
      "Best regularization penalty type:  l2\n",
      "Best regularization parameter:  2.7825594022071245\n"
     ]
    }
   ],
   "source": [
    "# prepare a uniform distribution to sample for the penalty & C hyperparameters\n",
    "penalty = ['l1', 'l2']\n",
    "C = np.logspace(0, 4, num=10)\n",
    "grid = dict(C=C, penalty=penalty)\n",
    "\n",
    "# create and fit a ridge regression model, testing random alpha values\n",
    "logistic = LogisticRegression()\n",
    "rsearch = RandomizedSearchCV(estimator=logistic, param_distributions=grid, n_iter=50, scoring = 'accuracy')\n",
    "\n",
    "rsearch.fit(X_train, y_train)\n",
    "\n",
    "# summarize the results of the grid search\n",
    "print('Accuracy: ', np.round(rsearch.best_score_, 2))\n",
    "print('Best regularization penalty type: ', rsearch.best_estimator_.penalty)\n",
    "print('Best regularization parameter: ', rsearch.best_estimator_.C)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pfa-venv",
   "language": "python",
   "name": "pfa-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
